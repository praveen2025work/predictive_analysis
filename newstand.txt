# Updated Standalone Market Analysis Report Generator
# standalone_market_report_updated.py

"""
Professional Financial Balance Analysis Report Generator
Updated for your specific data structure with SAP_BOOK_ID format
"""

import subprocess
import sys
import warnings
warnings.filterwarnings('ignore')

def install_and_import():
    """Install required packages and import modules"""
    
    # Install packages
    packages = ['pandas', 'numpy', 'matplotlib', 'scikit-learn']
    print("📦 Installing required packages...")
    
    for package in packages:
        try:
            print(f"   Installing {package}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", package, "--quiet"])
            print(f"   ✅ {package} installed")
        except:
            print(f"   ⚠️ Could not install {package}")
    
    # Import modules
    try:
        import pandas as pd
        import numpy as np
        import matplotlib.pyplot as plt
        from datetime import datetime, timedelta
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.preprocessing import StandardScaler
        from sklearn.model_selection import train_test_split
        from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
        from typing import Dict, List, Any
        import json
        import random
        
        print("✅ All modules imported successfully")
        return pd, np, plt, datetime, timedelta, RandomForestRegressor, StandardScaler, train_test_split, mean_absolute_error, mean_squared_error, r2_score, Dict, List, Any, json, random
    except ImportError as e:
        print(f"❌ Import error: {e}")
        print("Please install required packages manually: pip install pandas numpy matplotlib scikit-learn")
        sys.exit(1)

# Import all required modules
pd, np, plt, datetime, timedelta, RandomForestRegressor, StandardScaler, train_test_split, mean_absolute_error, mean_squared_error, r2_score, Dict, List, Any, json, random = install_and_import()

class EnhancedBalancePredictor:
    """Enhanced balance predictor using your data structure"""
    
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = [
            'DAYS_SINCE_EPOCH', 'JOURNAL_COUNT', 'JOURNAL_TOTAL_AMOUNT', 
            'JOURNAL_AVG_AMOUNT', 'JOURNAL_STD_AMOUNT', 'MONTH', 'QUARTER',
            'DAY_OF_WEEK', 'IS_WEEKEND', 'BALANCE_LAG_1', 'BALANCE_LAG_7',
            # Additional features from your data structure
            'CREDIT_COUNT', 'DEBIT_COUNT', 'ADJUSTMENT_COUNT',
            'SAP_ENTRIES_RATIO', 'ORACLE_ENTRIES_RATIO', 'MANUAL_ENTRIES_RATIO',
            'APPROVED_RATIO', 'AVG_POSTING_DELAY'
        ]
        self.is_trained = False
        self.feature_importance = None
    
    def generate_realistic_data_with_your_structure(self, n_years=7):
        """Generate realistic data matching your exact structure"""
        print("📊 Generating realistic data with your structure...")
        
        journal_entries = []
        balance_records = []
        transaction_types = ["Credit", "Debit", "Adjustment"]
        source_systems = ["SAP", "Oracle", "Manual"]
        remarks_samples = ["Initial entry", "Correction", "Adjustment", "Approved"]
        
        # Track balances per book
        book_balances = {}
        sap_book_ids = [f"SAP{i}" for i in range(1, 11)]  # SAP1 to SAP10
        
        # Initialize balances
        for book_id in sap_book_ids:
            book_balances[book_id] = random.uniform(50000, 150000)
        
        for year in range(2018, 2025):  # 7 years as in your data
            for month in range(1, 13):
                for day in range(1, 29):  # Safe day range
                    try:
                        entry_date = datetime(year, month, day)
                        
                        # Generate multiple entries per day
                        daily_entries = random.randint(3, 15)
                        
                        for _ in range(daily_entries):
                            book_id = random.choice(sap_book_ids)
                            
                            # Generate more realistic transaction values
                            transaction_type = random.choice(transaction_types)
                            if transaction_type == "Credit":
                                value = random.uniform(100, 5000)
                            elif transaction_type == "Debit":
                                value = -random.uniform(100, 3000)
                            else:  # Adjustment
                                value = random.uniform(-2000, 2000)
                            
                            # Add seasonal and business logic
                            # Month-end effects
                            if day > 25:
                                value *= 1.2
                            
                            # Year-end effects
                            if month == 12:
                                value *= 1.1
                            
                            # Update balance
                            book_balances[book_id] += value
                            daily_change = value
                            document_counter = random.randint(1, 9999)
                            
                            journal_entries.append({
                                "SAP_BOOK_ID": book_id,
                                "SAP_BOOK_NAME": f"Book_{book_id.replace('SAP', '')}",
                                "COST_CENTER": f"CC_{random.randint(1000, 9999)}",
                                "TRANSACTION_CURRENCY": "USD",
                                "VALUE": round(value, 2),
                                "ENTRY_DATE": entry_date,
                                "POSTING_DATE": entry_date + timedelta(days=random.randint(0, 3)),
                                "USERNAME": f"user_{random.randint(1, 50)}",
                                "DOCUMENT_NUMBER": f"DOC{entry_date.strftime('%Y%m%d')}{str(document_counter).zfill(4)}",
                                "TRANSACTION_TYPE": transaction_type,
                                "POSTED_BY": f"user_{random.randint(1, 20)}",
                                "APPROVED_BY": None if random.random() < 0.2 else f"manager_{random.randint(1, 5)}",
                                "CREATED_TIMESTAMP": entry_date,
                                "UPDATED_TIMESTAMP": entry_date + timedelta(days=random.randint(1, 5)),
                                "SOURCE_SYSTEM": random.choice(source_systems),
                                "REMARKS": random.choice(remarks_samples)
                            })
                            
                            # Create balance record for this book and date
                            balance_records.append({
                                "SAP_BOOK_ID": book_id,
                                "DATE": entry_date,
                                "BALANCE": round(book_balances[book_id], 2),
                                "DAILY_CHANGE": round(daily_change, 2),
                                "TOTAL_JOURNALS": len([j for j in journal_entries if j["SAP_BOOK_ID"] == book_id]),
                                "LAST_UPDATED_BY": f"user_{random.randint(1, 50)}"
                            })
                    except ValueError:
                        continue  # Skip invalid dates
        
        print(f"✅ Generated {len(journal_entries)} journal entries")
        print(f"✅ Generated {len(balance_records)} balance records")
        print(f"📅 Date range: 2018-01-01 to 2024-12-31")
        
        return pd.DataFrame(journal_entries), pd.DataFrame(balance_records)
    
    def load_data_from_files(self):
        """Load data from your generated JSON files"""
        try:
            print("📂 Loading data from JSON files...")
            
            # Load journal entries
            with open('journal_entries.json', 'r') as f:
                journal_data = json.load(f)
            
            # Load balance records
            with open('balance_records.json', 'r') as f:
                balance_data = json.load(f)
            
            journal_df = pd.DataFrame(journal_data)
            balance_df = pd.DataFrame(balance_data)
            
            print(f"✅ Loaded {len(journal_df)} journal entries")
            print(f"✅ Loaded {len(balance_df)} balance records")
            
            return journal_df, balance_df
            
        except FileNotFoundError as e:
            print(f"⚠️ Data files not found: {e}")
            print("🔄 Generating new data...")
            return self.generate_realistic_data_with_your_structure()
    
    def create_advanced_features(self, balance_df, journal_df):
        """Create advanced features from your data structure"""
        print("🔧 Creating advanced features...")
        
        # Convert date columns
        balance_df['DATE'] = pd.to_datetime(balance_df['DATE'])
        journal_df['ENTRY_DATE'] = pd.to_datetime(journal_df['ENTRY_DATE'])
        journal_df['POSTING_DATE'] = pd.to_datetime(journal_df['POSTING_DATE'])
        
        # Basic time features
        balance_df['YEAR'] = balance_df['DATE'].dt.year
        balance_df['MONTH'] = balance_df['DATE'].dt.month
        balance_df['QUARTER'] = balance_df['DATE'].dt.quarter
        balance_df['DAY_OF_WEEK'] = balance_df['DATE'].dt.dayofweek
        balance_df['IS_WEEKEND'] = (balance_df['DAY_OF_WEEK'] >= 5).astype(int)
        balance_df['DAYS_SINCE_EPOCH'] = (balance_df['DATE'] - pd.Timestamp('1970-01-01')).dt.days
        
        # Journal aggregations by date and book
        print("📊 Aggregating journal data...")
        journal_agg = journal_df.groupby(['SAP_BOOK_ID', journal_df['ENTRY_DATE'].dt.date]).agg({
            'VALUE': ['sum', 'mean', 'std', 'count'],
            'TRANSACTION_TYPE': lambda x: list(x),
            'SOURCE_SYSTEM': lambda x: list(x),
            'APPROVED_BY': lambda x: sum(1 for v in x if v is not None)
        }).reset_index()
        
        # Flatten column names
        journal_agg.columns = [
            'SAP_BOOK_ID', 'DATE', 'JOURNAL_TOTAL_AMOUNT', 'JOURNAL_AVG_AMOUNT', 
            'JOURNAL_STD_AMOUNT', 'JOURNAL_COUNT', 'TRANSACTION_TYPES', 
            'SOURCE_SYSTEMS', 'APPROVED_COUNT'
        ]
        
        # Convert date back to datetime
        journal_agg['DATE'] = pd.to_datetime(journal_agg['DATE'])
        
        # Fill NaN standard deviations
        journal_agg['JOURNAL_STD_AMOUNT'] = journal_agg['JOURNAL_STD_AMOUNT'].fillna(0)
        
        # Create transaction type features
        def count_transaction_type(types_list, target_type):
            return sum(1 for t in types_list if t == target_type)
        
        journal_agg['CREDIT_COUNT'] = journal_agg['TRANSACTION_TYPES'].apply(
            lambda x: count_transaction_type(x, 'Credit')
        )
        journal_agg['DEBIT_COUNT'] = journal_agg['TRANSACTION_TYPES'].apply(
            lambda x: count_transaction_type(x, 'Debit')
        )
        journal_agg['ADJUSTMENT_COUNT'] = journal_agg['TRANSACTION_TYPES'].apply(
            lambda x: count_transaction_type(x, 'Adjustment')
        )
        
        # Create source system features
        def count_source_system(systems_list, target_system):
            return sum(1 for s in systems_list if s == target_system)
        
        journal_agg['SAP_ENTRIES_RATIO'] = journal_agg.apply(
            lambda row: count_source_system(row['SOURCE_SYSTEMS'], 'SAP') / max(row['JOURNAL_COUNT'], 1), axis=1
        )
        journal_agg['ORACLE_ENTRIES_RATIO'] = journal_agg.apply(
            lambda row: count_source_system(row['SOURCE_SYSTEMS'], 'Oracle') / max(row['JOURNAL_COUNT'], 1), axis=1
        )
        journal_agg['MANUAL_ENTRIES_RATIO'] = journal_agg.apply(
            lambda row: count_source_system(row['SOURCE_SYSTEMS'], 'Manual') / max(row['JOURNAL_COUNT'], 1), axis=1
        )
        
        # Approval ratio
        journal_agg['APPROVED_RATIO'] = journal_agg['APPROVED_COUNT'] / journal_agg['JOURNAL_COUNT']
        
        # Calculate average posting delay
        posting_delays = journal_df.groupby(['SAP_BOOK_ID', journal_df['ENTRY_DATE'].dt.date]).apply(
            lambda x: (x['POSTING_DATE'] - x['ENTRY_DATE']).dt.days.mean()
        ).reset_index()
        posting_delays.columns = ['SAP_BOOK_ID', 'DATE', 'AVG_POSTING_DELAY']
        posting_delays['DATE'] = pd.to_datetime(posting_delays['DATE'])
        
        # Merge posting delays
        journal_agg = journal_agg.merge(posting_delays, on=['SAP_BOOK_ID', 'DATE'], how='left')
        journal_agg['AVG_POSTING_DELAY'] = journal_agg['AVG_POSTING_DELAY'].fillna(0)
        
        # Drop intermediate columns
        journal_agg = journal_agg.drop(['TRANSACTION_TYPES', 'SOURCE_SYSTEMS', 'APPROVED_COUNT'], axis=1)
        
        # Merge with balance data
        merged_df = pd.merge(balance_df, journal_agg, on=['SAP_BOOK_ID', 'DATE'], how='left')
        
        # Fill missing journal features with 0
        journal_features = [
            'JOURNAL_TOTAL_AMOUNT', 'JOURNAL_AVG_AMOUNT', 'JOURNAL_STD_AMOUNT', 'JOURNAL_COUNT',
            'CREDIT_COUNT', 'DEBIT_COUNT', 'ADJUSTMENT_COUNT', 'SAP_ENTRIES_RATIO', 
            'ORACLE_ENTRIES_RATIO': 0.3 if 'oracle' in test_case.lower() else 0.2,
            'MANUAL_ENTRIES_RATIO': 0.1 if 'manual' in test_case.lower() else 0.2,
            'APPROVED_RATIO': 0.8 if 'approved' in test_case.lower() else 0.7,
            'AVG_POSTING_DELAY': 1.0,
            'BALANCE_LAG_1': 100000,
            'BALANCE_LAG_7': 98000
        }
        
        # Calculate average amount
        if input_data['JOURNAL_COUNT'] > 0:
            input_data['JOURNAL_AVG_AMOUNT'] = input_data['JOURNAL_TOTAL_AMOUNT'] / input_data['JOURNAL_COUNT']
        
        try:
            prediction = predictor.predict_enhanced(input_data)
            print(f"   ✅ Predicted Balance: ${prediction:,.2f}")
            print(f"   📊 Input: {input_data['JOURNAL_COUNT']} entries, ${input_data['JOURNAL_TOTAL_AMOUNT']:,.0f} total")
            print(f"   🏦 Book: {sap_book_id}, Credits: {credit_count}, Debits: {debit_count}")
        except Exception as e:
            print(f"   ❌ Prediction failed: {e}")

class EnhancedReportGenerator:
    """Enhanced report generator for your data structure"""
    
    def __init__(self, predictor: EnhancedBalancePredictor):
        self.predictor = predictor
        self.current_date = datetime.now()
    
    def analyze_sap_book_performance(self, df):
        """Analyze performance by SAP book ID"""
        if 'SAP_BOOK_ID' not in df.columns:
            return {}
        
        book_analysis = {}
        
        for book_id in df['SAP_BOOK_ID'].unique():
            book_data = df[df['SAP_BOOK_ID'] == book_id]
            
            book_analysis[book_id] = {
                'avg_balance': book_data['BALANCE'].mean(),
                'balance_trend': self._calculate_trend(book_data['BALANCE']),
                'total_transactions': book_data['JOURNAL_COUNT'].sum(),
                'avg_transaction_size': book_data['JOURNAL_AVG_AMOUNT'].mean(),
                'volatility': book_data['BALANCE'].std(),
                'credit_ratio': book_data['CREDIT_COUNT'].sum() / max(book_data['JOURNAL_COUNT'].sum(), 1),
                'approval_ratio': book_data['APPROVED_RATIO'].mean() if 'APPROVED_RATIO' in book_data.columns else 0
            }
        
        return book_analysis
    
    def analyze_transaction_patterns(self, df):
        """Analyze transaction patterns specific to your data"""
        patterns = {}
        
        if 'CREDIT_COUNT' in df.columns:
            patterns['transaction_distribution'] = {
                'avg_credits_per_day': df['CREDIT_COUNT'].mean(),
                'avg_debits_per_day': df['DEBIT_COUNT'].mean(),
                'avg_adjustments_per_day': df['ADJUSTMENT_COUNT'].mean(),
                'credit_to_debit_ratio': df['CREDIT_COUNT'].sum() / max(df['DEBIT_COUNT'].sum(), 1)
            }
        
        if 'SAP_ENTRIES_RATIO' in df.columns:
            patterns['source_system_analysis'] = {
                'sap_dominance': df['SAP_ENTRIES_RATIO'].mean(),
                'oracle_usage': df['ORACLE_ENTRIES_RATIO'].mean(),
                'manual_processing': df['MANUAL_ENTRIES_RATIO'].mean()
            }
        
        if 'APPROVED_RATIO' in df.columns:
            patterns['approval_analysis'] = {
                'overall_approval_rate': df['APPROVED_RATIO'].mean(),
                'approval_trend': self._calculate_trend(df['APPROVED_RATIO'])
            }
        
        if 'AVG_POSTING_DELAY' in df.columns:
            patterns['processing_efficiency'] = {
                'avg_posting_delay': df['AVG_POSTING_DELAY'].mean(),
                'delay_trend': self._calculate_trend(df['AVG_POSTING_DELAY'])
            }
        
        return patterns
    
    def generate_enhanced_report(self, df):
        """Generate enhanced report with your data insights"""
        print("📋 Generating Enhanced Market Analysis Report...")
        
        current_analysis = self._analyze_current_performance(df)
        book_analysis = self.analyze_sap_book_performance(df)
        transaction_patterns = self.analyze_transaction_patterns(df)
        predictions = self._generate_predictions(df)
        
        report_date = datetime.now().strftime("%B %d, %Y")
        
        report = f"""
╔══════════════════════════════════════════════════════════════════════════════════════════════════════════════════╗
║                                    ENHANCED SAP FINANCIAL ANALYSIS REPORT                                         ║
║                                           {report_date}                                                    ║
╚══════════════════════════════════════════════════════════════════════════════════════════════════════════════════╝

EXECUTIVE SUMMARY
═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════

Analysis Period: {current_analysis.get('period', 'N/A')}
Total SAP Books Analyzed: {len(book_analysis)}
Records Processed: {len(df):,}

KEY METRICS:
• Portfolio Average Balance: ${current_analysis.get('avg_balance', 0):,.2f}
• Total Transaction Volume: ${current_analysis.get('total_volume', 0):,.2f}
• Average Daily Transactions: {current_analysis.get('avg_daily_transactions', 0):.0f}
• Overall Approval Rate: {transaction_patterns.get('approval_analysis', {}).get('overall_approval_rate', 0):.1%}

═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════
1. SAP BOOK PERFORMANCE ANALYSIS
═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════
"""

        # Add top performing books
        if book_analysis:
            sorted_books = sorted(book_analysis.items(), key=lambda x: x[1]['avg_balance'], reverse=True)
            
            report += "\nTOP PERFORMING SAP BOOKS:\n"
            for i, (book_id, metrics) in enumerate(sorted_books[:5], 1):
                report += f"""
{i}. {book_id}:
   • Average Balance: ${metrics['avg_balance']:,.2f}
   • Balance Trend: {metrics['balance_trend']}
   • Credit Ratio: {metrics['credit_ratio']:.1%}
   • Approval Rate: {metrics['approval_ratio']:.1%}"""

        report += f"""

TRANSACTION PATTERN ANALYSIS:
═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════
"""

        if 'transaction_distribution' in transaction_patterns:
            trans_dist = transaction_patterns['transaction_distribution']
            report += f"""
TRANSACTION TYPE DISTRIBUTION:
• Average Credits per Day: {trans_dist['avg_credits_per_day']:.1f}
• Average Debits per Day: {trans_dist['avg_debits_per_day']:.1f}  
• Average Adjustments per Day: {trans_dist['avg_adjustments_per_day']:.1f}
• Credit-to-Debit Ratio: {trans_dist['credit_to_debit_ratio']:.2f}"""

        if 'source_system_analysis' in transaction_patterns:
            source_analysis = transaction_patterns['source_system_analysis']
            report += f"""

SOURCE SYSTEM UTILIZATION:
• SAP System Usage: {source_analysis['sap_dominance']:.1%}
• Oracle System Usage: {source_analysis['oracle_usage']:.1%}
• Manual Processing: {source_analysis['manual_processing']:.1%}"""

        if 'processing_efficiency' in transaction_patterns:
            efficiency = transaction_patterns['processing_efficiency']
            report += f"""

PROCESSING EFFICIENCY:
• Average Posting Delay: {efficiency['avg_posting_delay']:.1f} days
• Delay Trend: {efficiency['delay_trend']}"""

        report += f"""

═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════
2. PREDICTIVE INSIGHTS & RECOMMENDATIONS  
═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════

MODEL PERFORMANCE:
• Prediction Accuracy (R²): {self.predictor.feature_importance is not None}
• Key Predictive Factors: {', '.join(self.predictor.feature_importance.head(3)['feature'].tolist()) if self.predictor.feature_importance is not None else 'N/A'}

STRATEGIC RECOMMENDATIONS:

1. TRANSACTION OPTIMIZATION:
   • Focus on books with high credit-to-debit ratios for cash flow optimization
   • Investigate books with low approval rates for process improvement

2. SYSTEM EFFICIENCY:
   • Consider increasing automation to reduce manual processing
   • Monitor posting delays to improve operational efficiency

3. RISK MANAGEMENT:
   • Implement enhanced monitoring for books with high volatility
   • Establish approval rate benchmarks across all SAP books

4. PROCESS IMPROVEMENT:
   • Standardize transaction processing across source systems
   • Implement real-time balance monitoring for critical books

═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════
TECHNICAL DETAILS
═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════

Model Features Used: {len(self.predictor.feature_columns)} advanced features
Data Quality: {(1 - df.isnull().sum().sum() / (len(df) * len(df.columns))):.1%} complete
Processing Period: {df['DATE'].min().strftime('%Y-%m-%d') if 'DATE' in df.columns else 'N/A'} to {df['DATE'].max().strftime('%Y-%m-%d') if 'DATE' in df.columns else 'N/A'}

Report Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
Model Version: Enhanced Random Forest v2.0

═══════════════════════════════════════════════════════════════════════════════════════════════════════════════════
"""
        
        return report
    
    def _analyze_current_performance(self, df):
        """Analyze current performance"""
        latest_month = df['DATE'].max().to_period('M') if 'DATE' in df.columns else None
        
        if latest_month:
            current_data = df[df['DATE'].dt.to_period('M') == latest_month]
        else:
            current_data = df.tail(1000)  # Last 1000 records
        
        return {
            'period': str(latest_month) if latest_month else 'Latest Data',
            'avg_balance': current_data['BALANCE'].mean(),
            'total_volume': current_data['JOURNAL_TOTAL_AMOUNT'].sum(),
            'avg_daily_transactions': current_data['JOURNAL_COUNT'].mean()
        }
    
    def _generate_predictions(self, df):
        """Generate predictions for key scenarios"""
        if not self.predictor.is_trained:
            return {}
        
        # Sample prediction scenarios
        scenarios = [
            {
                'name': 'High Activity Day',
                'data': {
                    'JOURNAL_COUNT': 25, 'JOURNAL_TOTAL_AMOUNT': 100000,
                    'CREDIT_COUNT': 15, 'DEBIT_COUNT': 8, 'ADJUSTMENT_COUNT': 2
                }
            },
            {
                'name': 'Normal Business Day', 
                'data': {
                    'JOURNAL_COUNT': 12, 'JOURNAL_TOTAL_AMOUNT': 45000,
                    'CREDIT_COUNT': 8, 'DEBIT_COUNT': 3, 'ADJUSTMENT_COUNT': 1
                }
            }
        ]
        
        predictions = {}
        for scenario in scenarios:
            try:
                pred = self.predictor.predict_enhanced(scenario['data'])
                predictions[scenario['name']] = pred
            except:
                predictions[scenario['name']] = 'N/A'
        
        return predictions
    
    def _calculate_trend(self, data_series):
        """Calculate trend direction"""
        if len(data_series) < 2:
            return "Insufficient data"
        
        try:
            x = np.arange(len(data_series))
            slope = np.polyfit(x, data_series.dropna().values, 1)[0]
            
            if abs(slope) < data_series.std() * 0.01:
                return "Stable"
            elif slope > 0:
                return f"Increasing (+{abs(slope):.0f}/period)"
            else:
                return f"Decreasing (-{abs(slope):.0f}/period)"
        except:
            return "Unable to determine"

def main():
    """Main execution function"""
    print("🚀 ENHANCED SAP FINANCIAL ANALYSIS SYSTEM")
    print("=" * 70)
    print("Designed for your SAP Book ID data structure with advanced features")
    print("=" * 70)
    
    try:
        # Initialize predictor
        print("\n1️⃣ Initializing Enhanced Balance Predictor...")
        predictor = EnhancedBalancePredictor()
        
        # Load or generate data
        print("\n2️⃣ Loading financial data...")
        journal_df, balance_df = predictor.load_data_from_files()
        
        # Create enhanced features
        print("\n3️⃣ Creating enhanced features...")
        enhanced_df = predictor.create_advanced_features(balance_df, journal_df)
        
        # Train model
        print("\n4️⃣ Training enhanced Random Forest model...")
        training_results = predictor.train_enhanced_model(enhanced_df)
        
        # Create visualizations
        print("\n5️⃣ Creating enhanced visualizations...")
        predictor.create_enhanced_visualizations(training_results)
        
        # Test enhanced predictions
        test_enhanced_predictions(predictor)
        
        # Generate enhanced report
        print("\n6️⃣ Generating enhanced market analysis report...")
        report_generator = EnhancedReportGenerator(predictor)
        report = report_generator.generate_enhanced_report(enhanced_df)
        
        # Display report
        print("\n" + "="*100)
        print("📋 ENHANCED SAP FINANCIAL ANALYSIS REPORT")
        print("="*100)
        print(report)
        
        # Save report
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"enhanced_sap_analysis_report_{timestamp}.txt"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"\n📄 Enhanced report saved to: {filename}")
        except Exception as e:
            print(f"❌ Error saving report: {e}")
        
        print(f"\n✅ Enhanced analysis complete!")
        print(f"📊 Analyzed {len(enhanced_df)} records across {enhanced_df['SAP_BOOK_ID'].nunique()} SAP books")
        print(f"🤖 Model accuracy (R²): {training_results['r2']:.1%}")
        print(f"🔍 Used {len(training_results['available_features'])} advanced features")
        
        # Summary of capabilities
        print(f"\n🎯 Enhanced System Capabilities:")
        print(f"   • SAP Book-specific analysis")
        print(f"   • Transaction type pattern recognition") 
        print(f"   • Source system utilization tracking")
        print(f"   • Approval workflow analysis")
        print(f"   • Processing efficiency metrics")
        print(f"   • Advanced ML predictions with {len(predictor.feature_columns)} features")
        
        return predictor, report_generator, enhanced_df, report, training_results
        
    except Exception as e:
        print(f"\n❌ Error in enhanced analysis: {e}")
        print("Please check that your data files are available or run the data generation script first.")
        return None

# Custom prediction function for your data structure
def predict_for_sap_book(predictor, book_id, transaction_counts, total_amount, day_type="weekday"):
    """
    Custom prediction function for your SAP book structure
    
    Args:
        predictor: Trained predictor instance
        book_id: SAP book ID (e.g., "SAP5")
        transaction_counts: Dict with 'credits', 'debits', 'adjustments'
        total_amount: Total transaction amount
        day_type: "weekday", "weekend", "month_end"
    """
    
    if not predictor.is_trained:
        print("❌ Model not trained yet")
        return None
    
    # Calculate derived values
    total_transactions = sum(transaction_counts.values())
    avg_amount = total_amount / max(total_transactions, 1)
    
    # Day type mapping
    day_mapping = {
        "weekday": {"day_of_week": 1, "is_weekend": 0},
        "weekend": {"day_of_week": 5, "is_weekend": 1}, 
        "month_end": {"day_of_week": 1, "is_weekend": 0}  # Typically busier
    }
    
    day_info = day_mapping.get(day_type, day_mapping["weekday"])
    
    # Enhanced input with your data structure
    input_data = {
        'DAYS_SINCE_EPOCH': (datetime.now() - datetime(1970, 1, 1)).days,
        'JOURNAL_COUNT': total_transactions,
        'JOURNAL_TOTAL_AMOUNT': total_amount,
        'JOURNAL_AVG_AMOUNT': avg_amount,
        'JOURNAL_STD_AMOUNT': avg_amount * 0.3,  # Estimated
        'MONTH': datetime.now().month,
        'QUARTER': (datetime.now().month - 1) // 3 + 1,
        'DAY_OF_WEEK': day_info["day_of_week"],
        'IS_WEEKEND': day_info["is_weekend"],
        'CREDIT_COUNT': transaction_counts.get('credits', 0),
        'DEBIT_COUNT': transaction_counts.get('debits', 0),
        'ADJUSTMENT_COUNT': transaction_counts.get('adjustments', 0),
        'SAP_ENTRIES_RATIO': 0.7,  # Assuming most entries are SAP
        'ORACLE_ENTRIES_RATIO': 0.2,
        'MANUAL_ENTRIES_RATIO': 0.1,
        'APPROVED_RATIO': 0.85,  # Typical approval rate
        'AVG_POSTING_DELAY': 1.0,
        'BALANCE_LAG_1': 150000,  # Estimated previous balance
        'BALANCE_LAG_7': 145000   # Estimated week-ago balance
    }
    
    try:
        prediction = predictor.predict_enhanced(input_data)
        
        print(f"🏦 SAP Book: {book_id}")
        print(f"📊 Transactions: {total_transactions} ({transaction_counts})")
        print(f"💰 Total Amount: ${total_amount:,.2f}")
        print(f"📅 Day Type: {day_type}")
        print(f"🎯 Predicted Balance: ${prediction:,.2f}")
        
        return prediction
        
    except Exception as e:
        print(f"❌ Prediction error: {e}")
        return None

# Example usage function
def demo_enhanced_predictions():
    """Demonstrate enhanced predictions with your data structure"""
    print("\n🎯 ENHANCED PREDICTION DEMO")
    print("=" * 50)
    
    # Example scenarios for your SAP books
    scenarios = [
        {
            'book_id': 'SAP5',
            'transaction_counts': {'credits': 12, 'debits': 5, 'adjustments': 2},
            'total_amount': 75000,
            'day_type': 'weekday'
        },
        {
            'book_id': 'SAP3', 
            'transaction_counts': {'credits': 8, 'debits': 8, 'adjustments': 4},
            'total_amount': 50000,
            'day_type': 'month_end'
        },
        {
            'book_id': 'SAP1',
            'transaction_counts': {'credits': 20, 'debits': 3, 'adjustments': 1}, 
            'total_amount': 120000,
            'day_type': 'weekend'
        }
    ]
    
    return scenarios

if __name__ == "__main__":
    # Run main analysis
    results = main()
    
    if results:
        predictor, generator, data, report, training_results = results
        
        # Demo enhanced predictions
        demo_scenarios = demo_enhanced_predictions()
        
        print(f"\n💡 Try custom predictions:")
        print(f"   scenarios = demo_enhanced_predictions()")
        print(f"   for scenario in scenarios:")
        print(f"       predict_for_sap_book(predictor, **scenario)")
        
        print(f"\n🎉 SUCCESS! Enhanced SAP analysis system is ready.")
        print(f"📈 Your model achieved {training_results['r2']:.1%} accuracy")
        print(f"🏦 Analyzed {data['SAP_BOOK_ID'].nunique()} different SAP books")
        print(f"📊 Enhanced features provide deeper insights into your financial patterns")
    else:
        print("❌ Enhanced analysis failed. Please check your data files.")
_RATIO', 'MANUAL_ENTRIES_RATIO', 'APPROVED_RATIO', 'AVG_POSTING_DELAY'
        ]
        
        for feature in journal_features:
            merged_df[feature] = merged_df[feature].fillna(0)
        
        # Create lag features
        merged_df = merged_df.sort_values(['SAP_BOOK_ID', 'DATE'])
        merged_df['BALANCE_LAG_1'] = merged_df.groupby('SAP_BOOK_ID')['BALANCE'].shift(1)
        merged_df['BALANCE_LAG_7'] = merged_df.groupby('SAP_BOOK_ID')['BALANCE'].shift(7)
        
        # Create rolling features
        for window in [7, 30]:
            merged_df[f'BALANCE_ROLLING_MEAN_{window}'] = merged_df.groupby('SAP_BOOK_ID')['BALANCE'].rolling(
                window=window, min_periods=1
            ).mean().reset_index(0, drop=True)
            
            merged_df[f'JOURNAL_ROLLING_MEAN_{window}'] = merged_df.groupby('SAP_BOOK_ID')['JOURNAL_TOTAL_AMOUNT'].rolling(
                window=window, min_periods=1
            ).mean().reset_index(0, drop=True)
        
        # Fill remaining missing values
        merged_df = merged_df.fillna(method='ffill').fillna(0)
        
        print(f"✅ Created enhanced dataset with {len(merged_df)} records")
        print(f"📊 Features: {len([col for col in merged_df.columns if col in self.feature_columns])}")
        
        return merged_df
    
    def train_enhanced_model(self, df):
        """Train Random Forest model with enhanced features"""
        print("🤖 Training Enhanced Random Forest model...")
        
        # Prepare features - only use columns that exist
        available_features = [col for col in self.feature_columns if col in df.columns]
        print(f"📊 Using {len(available_features)} features: {available_features}")
        
        # Split data by year
        train_df = df[df['YEAR'] <= 2022].copy()
        test_df = df[df['YEAR'] > 2022].copy()
        
        if len(test_df) == 0:
            print("⚠️ No recent data for testing, using random split")
            train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
        
        # Remove rows with missing target
        train_df = train_df.dropna(subset=['BALANCE'])
        test_df = test_df.dropna(subset=['BALANCE'])
        
        # Prepare features and target
        X_train = train_df[available_features].fillna(0)
        y_train = train_df['BALANCE']
        X_test = test_df[available_features].fillna(0)
        y_test = test_df['BALANCE']
        
        print(f"📈 Training set: {len(X_train)} samples")
        print(f"📉 Test set: {len(X_test)} samples")
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Train model with optimized hyperparameters
        self.model = RandomForestRegressor(
            n_estimators=200,
            max_depth=25,
            min_samples_split=5,
            min_samples_leaf=2,
            max_features='sqrt',
            random_state=42,
            n_jobs=-1
        )
        
        self.model.fit(X_train_scaled, y_train)
        self.is_trained = True
        
        # Evaluate model
        y_pred = self.model.predict(X_test_scaled)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        
        # Feature importance
        self.feature_importance = pd.DataFrame({
            'feature': available_features,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        print(f"✅ Enhanced model trained successfully!")
        print(f"📊 Performance Metrics:")
        print(f"   • Mean Absolute Error: ${mae:,.2f}")
        print(f"   • Root Mean Square Error: ${rmse:,.2f}")
        print(f"   • R² Score: {r2:.3f}")
        
        print(f"\n🔍 Top 10 Most Important Features:")
        for i, (_, row) in enumerate(self.feature_importance.head(10).iterrows(), 1):
            print(f"   {i:2d}. {row['feature']:<25} {row['importance']:.4f}")
        
        return {
            'mae': mae,
            'rmse': rmse,
            'r2': r2,
            'feature_importance': self.feature_importance,
            'test_data': (X_test_scaled, y_test, y_pred, test_df),
            'available_features': available_features
        }
    
    def predict_enhanced(self, input_data: Dict[str, Any]) -> float:
        """Make prediction with enhanced features"""
        if not self.is_trained:
            raise ValueError("Model must be trained before making predictions")
        
        # Prepare feature vector with available features only
        available_features = [col for col in self.feature_columns if col in input_data or col in [
            'DAYS_SINCE_EPOCH', 'MONTH', 'QUARTER', 'DAY_OF_WEEK', 'IS_WEEKEND'
        ]]
        
        feature_vector = []
        for col in available_features:
            if col in input_data:
                feature_vector.append(input_data[col])
            else:
                # Default values for missing features
                default_values = {
                    'DAYS_SINCE_EPOCH': (datetime.now() - datetime(1970, 1, 1)).days,
                    'MONTH': datetime.now().month,
                    'QUARTER': (datetime.now().month - 1) // 3 + 1,
                    'DAY_OF_WEEK': datetime.now().weekday(),
                    'IS_WEEKEND': 1 if datetime.now().weekday() >= 5 else 0,
                }
                feature_vector.append(default_values.get(col, 0))
        
        # Scale and predict
        feature_vector = np.array(feature_vector).reshape(1, -1)
        feature_vector_scaled = self.scaler.transform(feature_vector)
        prediction = self.model.predict(feature_vector_scaled)[0]
        
        return round(prediction, 2)
    
    def create_enhanced_visualizations(self, training_results):
        """Create enhanced visualizations"""
        X_test_scaled, y_test, y_pred, test_df = training_results['test_data']
        
        plt.style.use('default')
        fig, axes = plt.subplots(3, 2, figsize=(15, 18))
        fig.suptitle('Enhanced Financial Balance Analysis Dashboard', fontsize=16, fontweight='bold')
        
        # 1. Actual vs Predicted
        axes[0, 0].scatter(y_test, y_pred, alpha=0.6, color='blue')
        axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        axes[0, 0].set_xlabel('Actual Balance')
        axes[0, 0].set_ylabel('Predicted Balance')
        axes[0, 0].set_title('Actual vs Predicted Balance')
        axes[0, 0].grid(True, alpha=0.3)
        
        # Add R² to the plot
        r2 = training_results['r2']
        axes[0, 0].text(0.05, 0.95, f'R² = {r2:.3f}', transform=axes[0, 0].transAxes, 
                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
        
        # 2. Feature Importance
        top_features = self.feature_importance.head(12)
        bars = axes[0, 1].barh(range(len(top_features)), top_features['importance'])
        axes[0, 1].set_yticks(range(len(top_features)))
        axes[0, 1].set_yticklabels(top_features['feature'])
        axes[0, 1].set_xlabel('Importance')
        axes[0, 1].set_title('Top 12 Feature Importance')
        axes[0, 1].grid(True, alpha=0.3)
        
        # Color bars with gradient
        colors = plt.cm.viridis(np.linspace(0, 1, len(bars)))
        for bar, color in zip(bars, colors):
            bar.set_color(color)
        
        # 3. Balance by SAP Book ID
        if 'SAP_BOOK_ID' in test_df.columns:
            book_balances = test_df.groupby('SAP_BOOK_ID')['BALANCE'].mean().sort_values(ascending=False)
            axes[1, 0].bar(range(len(book_balances)), book_balances.values, color='lightgreen')
            axes[1, 0].set_title('Average Balance by SAP Book ID')
            axes[1, 0].set_xlabel('SAP Book ID')
            axes[1, 0].set_ylabel('Average Balance ($)')
            axes[1, 0].set_xticks(range(len(book_balances)))
            axes[1, 0].set_xticklabels(book_balances.index, rotation=45)
            axes[1, 0].grid(True, alpha=0.3)
        
        # 4. Transaction Type Analysis
        if 'CREDIT_COUNT' in test_df.columns:
            credit_avg = test_df['CREDIT_COUNT'].mean()
            debit_avg = test_df['DEBIT_COUNT'].mean()
            adj_avg = test_df['ADJUSTMENT_COUNT'].mean()
            
            transaction_types = ['Credit', 'Debit', 'Adjustment']
            avg_counts = [credit_avg, debit_avg, adj_avg]
            
            axes[1, 1].pie(avg_counts, labels=transaction_types, autopct='%1.1f%%', startangle=90)
            axes[1, 1].set_title('Average Transaction Type Distribution')
        
        # 5. Time series plot
        if 'DATE' in test_df.columns:
            # Sample for readability
            sample_size = min(100, len(test_df))
            sample_indices = np.random.choice(len(test_df), sample_size, replace=False)
            sample_indices = sorted(sample_indices)
            
            sample_dates = test_df['DATE'].iloc[sample_indices]
            sample_actual = y_test.iloc[sample_indices]
            sample_pred = y_pred[sample_indices]
            
            # Sort by date
            sort_idx = sample_dates.argsort()
            axes[2, 0].plot(sample_dates.iloc[sort_idx], sample_actual.iloc[sort_idx], 
                           'o-', label='Actual', markersize=4, linewidth=1, color='blue')
            axes[2, 0].plot(sample_dates.iloc[sort_idx], sample_pred[sort_idx], 
                           's--', label='Predicted', markersize=4, linewidth=1, color='red')
            axes[2, 0].set_xlabel('Date')
            axes[2, 0].set_ylabel('Balance')
            axes[2, 0].set_title(f'Time Series Sample ({sample_size} points)')
            axes[2, 0].legend()
            axes[2, 0].grid(True, alpha=0.3)
            plt.setp(axes[2, 0].xaxis.get_majorticklabels(), rotation=45)
        
        # 6. Residuals analysis
        residuals = y_test - y_pred
        axes[2, 1].scatter(y_pred, residuals, alpha=0.6, color='purple')
        axes[2, 1].axhline(y=0, color='r', linestyle='--')
        axes[2, 1].set_xlabel('Predicted Balance')
        axes[2, 1].set_ylabel('Residuals')
        axes[2, 1].set_title('Residuals Analysis')
        axes[2, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        return fig

def test_enhanced_predictions(predictor):
    """Test enhanced natural language predictions"""
    print("\n🧠 Testing Enhanced Natural Language Prediction Interface...")
    print("=" * 70)
    
    test_cases = [
        "SAP5 book had 15 credit transactions worth $50,000 this Tuesday",
        "Book SAP3 processed 20 entries: 12 credits, 5 debits, 3 adjustments totaling $75,000",
        "SAP1 had 8 Oracle entries and 5 manual entries, total $25,000 on Friday",
        "Book SAP7: 30 transactions, 80% approved, $100,000 total, weekend processing"
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{i}. Testing: '{test_case}'")
        
        # Enhanced parsing for your data structure
        import re
        
        # Extract SAP book ID
        sap_match = re.search(r'SAP(\d+)', test_case)
        sap_book_id = sap_match.group(0) if sap_match else "SAP1"
        
        # Extract numbers
        numbers = re.findall(r'\d+(?:,\d{3})*(?:\.\d+)?', test_case)
        numbers = [float(n.replace(',', '')) for n in numbers]
        
        # Extract transaction types
        credit_count = 0
        debit_count = 0
        adjustment_count = 0
        
        if 'credit' in test_case.lower():
            credit_match = re.search(r'(\d+)\s*credit', test_case.lower())
            credit_count = int(credit_match.group(1)) if credit_match else len(numbers) // 2
        
        if 'debit' in test_case.lower():
            debit_match = re.search(r'(\d+)\s*debit', test_case.lower())
            debit_count = int(debit_match.group(1)) if debit_match else 0
        
        if 'adjustment' in test_case.lower():
            adj_match = re.search(r'(\d+)\s*adjustment', test_case.lower())
            adjustment_count = int(adj_match.group(1)) if adj_match else 0
        
        # Default values
        current_date = datetime.now()
        
        # Enhanced input data
        input_data = {
            'JOURNAL_COUNT': int(numbers[0]) if len(numbers) > 0 else 15,
            'JOURNAL_TOTAL_AMOUNT': numbers[-1] if len(numbers) > 1 else 50000,
            'JOURNAL_AVG_AMOUNT': 0,  # Will be calculated
            'JOURNAL_STD_AMOUNT': 500,
            'MONTH': current_date.month,
            'QUARTER': (current_date.month - 1) // 3 + 1,
            'DAY_OF_WEEK': 1 if 'tuesday' in test_case.lower() else 4 if 'friday' in test_case.lower() else current_date.weekday(),
            'IS_WEEKEND': 1 if 'weekend' in test_case.lower() else 0,
            'DAYS_SINCE_EPOCH': (current_date - datetime(1970, 1, 1)).days,
            'CREDIT_COUNT': credit_count,
            'DEBIT_COUNT': debit_count,
            'ADJUSTMENT_COUNT': adjustment_count,
            'SAP_ENTRIES_RATIO': 0.6,
            'ORACLE_ENTRIES
