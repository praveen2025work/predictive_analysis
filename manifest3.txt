# train_and_upload_model.py - Enhanced version for your project
import os
import joblib
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from azure.storage.blob import BlobServiceClient
import json
import logging
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ModelTrainerUploader:
    def __init__(self):
        # Azure configuration
        self.storage_connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')
        self.container_name = os.getenv('CONTAINER_NAME', 'journaldata')
        
        # Feature columns (must match app.py)
        self.feature_columns = [
            'DAYS_SINCE_EPOCH', 'JOURNAL_COUNT', 'JOURNAL_TOTAL_AMOUNT', 
            'JOURNAL_AVG_AMOUNT', 'JOURNAL_STD_AMOUNT', 'MONTH', 'QUARTER',
            'DAY_OF_WEEK', 'IS_WEEKEND', 'BALANCE_LAG_1', 'BALANCE_LAG_7'
        ]
        
        # Initialize Azure client
        if self.storage_connection_string:
            self.blob_service_client = BlobServiceClient.from_connection_string(
                self.storage_connection_string
            )
        else:
            self.blob_service_client = None
            logger.warning("No Azure storage connection string found")
        
        self.model = None
        self.scaler = StandardScaler()

    def create_sample_data(self):
        """Create sample financial data for training"""
        logger.info("üìä Creating sample financial data...")
        
        # Create sample balance records
        balance_records = []
        journal_entries = []
        
        start_date = datetime(2020, 1, 1)
        end_date = datetime(2023, 12, 31)
        
        current_date = start_date
        book_ids = ['BOOK_001', 'BOOK_002', 'BOOK_003']
        
        while current_date <= end_date:
            for book_id in book_ids:
                # Generate balance record
                base_balance = 100000 + np.random.normal(0, 20000)
                seasonal_factor = 1 + 0.1 * np.sin(2 * np.pi * current_date.timetuple().tm_yday / 365)
                balance = max(10000, base_balance * seasonal_factor)
                
                balance_records.append({
                    'DATE': current_date.isoformat(),
                    'SAP_BOOK_ID': book_id,
                    'BALANCE': round(balance, 2)
                })
                
                # Generate journal entries
                n_entries = max(1, int(np.random.poisson(8)))
                for _ in range(n_entries):
                    amount = np.random.normal(2000, 500) * seasonal_factor
                    journal_entries.append({
                        'ENTRY_DATE': current_date.isoformat(),
                        'SAP_BOOK_ID': book_id,
                        'VALUE': round(amount, 2)
                    })
            
            current_date += timedelta(days=1)
        
        logger.info(f"‚úÖ Created {len(balance_records)} balance records and {len(journal_entries)} journal entries")
        return balance_records, journal_entries

    def upload_sample_data(self, balance_records, journal_entries):
        """Upload sample data to Azure Blob Storage"""
        if not self.blob_service_client:
            logger.warning("No blob service client available")
            return False
        
        try:
            # Upload balance records
            balance_blob = self.blob_service_client.get_blob_client(
                container=self.container_name,
                blob='balance_records.json'
            )
            balance_blob.upload_blob(
                json.dumps(balance_records, indent=2),
                overwrite=True
            )
            
            # Upload journal entries
            journal_blob = self.blob_service_client.get_blob_client(
                container=self.container_name,
                blob='journal_entries.json'
            )
            journal_blob.upload_blob(
                json.dumps(journal_entries, indent=2),
                overwrite=True
            )
            
            logger.info("‚úÖ Sample data uploaded to Azure Blob Storage")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to upload sample data: {e}")
            return False

    def process_data_for_training(self, balance_records, journal_entries):
        """Process data for ML training"""
        logger.info("üîß Processing data for training...")
        
        # Convert to DataFrames
        balance_df = pd.DataFrame(balance_records)
        journal_df = pd.DataFrame(journal_entries)
        
        # Process balance data
        balance_df['DATE'] = pd.to_datetime(balance_df['DATE'])
        balance_df['BALANCE'] = pd.to_numeric(balance_df['BALANCE'])
        balance_df['YEAR'] = balance_df['DATE'].dt.year
        
        # Process and aggregate journal data
        journal_df['ENTRY_DATE'] = pd.to_datetime(journal_df['ENTRY_DATE'])
        journal_df['VALUE'] = pd.to_numeric(journal_df['VALUE'])
        
        journal_agg = journal_df.groupby([journal_df['ENTRY_DATE'].dt.date, 'SAP_BOOK_ID']).agg({
            'VALUE': ['sum', 'mean', 'count', 'std']
        }).reset_index()
        
        # Flatten column names
        journal_agg.columns = ['ENTRY_DATE', 'SAP_BOOK_ID', 'JOURNAL_TOTAL_AMOUNT', 
                              'JOURNAL_AVG_AMOUNT', 'JOURNAL_COUNT', 'JOURNAL_STD_AMOUNT']
        journal_agg['DATE'] = pd.to_datetime(journal_agg['ENTRY_DATE'])
        journal_agg.drop(columns=['ENTRY_DATE'], inplace=True)
        journal_agg['JOURNAL_STD_AMOUNT'].fillna(0, inplace=True)
        
        # Merge datasets
        merged_df = pd.merge(balance_df, journal_agg, how='left', on=['DATE', 'SAP_BOOK_ID'])
        
        # Create features
        merged_df['MONTH'] = merged_df['DATE'].dt.month
        merged_df['QUARTER'] = merged_df['DATE'].dt.quarter
        merged_df['DAY_OF_WEEK'] = merged_df['DATE'].dt.dayofweek
        merged_df['IS_WEEKEND'] = (merged_df['DAY_OF_WEEK'] >= 5).astype(int)
        merged_df['DAYS_SINCE_EPOCH'] = (merged_df['DATE'] - pd.Timestamp('1970-01-01')).dt.days
        
        # Create lag features
        merged_df = merged_df.sort_values(['SAP_BOOK_ID', 'DATE'])
        merged_df['BALANCE_LAG_1'] = merged_df.groupby('SAP_BOOK_ID')['BALANCE'].shift(1)
        merged_df['BALANCE_LAG_7'] = merged_df.groupby('SAP_BOOK_ID')['BALANCE'].shift(7)
        
        # Fill missing values
        merged_df.fillna(method='ffill', inplace=True)
        merged_df.fillna(0, inplace=True)
        
        logger.info(f"‚úÖ Data processed: {len(merged_df)} records ready for training")
        return merged_df

    def train_model(self, df):
        """Train Random Forest model"""
        logger.info("ü§ñ Training Random Forest model...")
        
        # Prepare features
        feature_cols = [col for col in self.feature_columns if col in df.columns]
        
        # Split data
        train_df = df[df['YEAR'] <= 2022].copy()
        test_df = df[df['YEAR'] > 2022].copy()
        
        if len(test_df) == 0:
            logger.warning("No test data available, using random split")
            train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
        
        # Prepare features and targets
        X_train = train_df[feature_cols].fillna(0)
        y_train = train_df['BALANCE']
        X_test = test_df[feature_cols].fillna(0)
        y_test = test_df['BALANCE']
        
        logger.info(f"Training set: {len(X_train)} samples")
        logger.info(f"Test set: {len(X_test)} samples")
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Train model
        self.model = RandomForestRegressor(
            n_estimators=150,
            max_depth=15,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        self.model.fit(X_train_scaled, y_train)
        
        # Evaluate
        y_pred = self.model.predict(X_test_scaled)
        mae = mean_absolute_error(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        r2 = r2_score(y_test, y_pred)
        
        logger.info(f"‚úÖ Model trained successfully!")
        logger.info(f"üìä Performance Metrics:")
        logger.info(f"   ‚Ä¢ MAE: ${mae:,.2f}")
        logger.info(f"   ‚Ä¢ RMSE: ${rmse:,.2f}")
        logger.info(f"   ‚Ä¢ R¬≤ Score: {r2:.3f}")
        
        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        logger.info(f"üîç Top 5 Feature Importance:")
        for i, (_, row) in enumerate(feature_importance.head().iterrows(), 1):
            logger.info(f"   {i}. {row['feature']}: {row['importance']:.3f}")
        
        return {'mae': mae, 'rmse': rmse, 'r2': r2, 'feature_importance': feature_importance}

    def upload_models_to_azure(self):
        """Upload trained model and scaler to Azure"""
        if not self.blob_service_client:
            logger.warning("No blob service client available")
            return False
        
        logger.info("‚òÅÔ∏è Uploading models to Azure...")
        
        try:
            # Save model locally first
            model_filename = 'trained_model.pkl'
            scaler_filename = 'feature_scaler.pkl'
            
            joblib.dump(self.model, model_filename)
            joblib.dump(self.scaler, scaler_filename)
            
            # Upload model
            with open(model_filename, 'rb') as model_file:
                model_blob = self.blob_service_client.get_blob_client(
                    container=self.container_name,
                    blob='trained_model.pkl'
                )
                model_blob.upload_blob(model_file, overwrite=True)
            
            # Upload scaler
            with open(scaler_filename, 'rb') as scaler_file:
                scaler_blob = self.blob_service_client.get_blob_client(
                    container=self.container_name,
                    blob='feature_scaler.pkl'
                )
                scaler_blob.upload_blob(scaler_file, overwrite=True)
            
            # Clean up local files
            os.remove(model_filename)
            os.remove(scaler_filename)
            
            logger.info("‚úÖ Models uploaded to Azure successfully!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to upload models: {e}")
            return False

    def test_uploaded_model(self):
        """Test the uploaded model"""
        if not self.blob_service_client:
            return False
        
        logger.info("üß™ Testing uploaded model...")
        
        try:
            # Download and test model
            model_blob = self.blob_service_client.get_blob_client(
                container=self.container_name,
                blob='trained_model.pkl'
            )
            model_data = model_blob.download_blob().readall()
            
            scaler_blob = self.blob_service_client.get_blob_client(
                container=self.container_name,
                blob='feature_scaler.pkl'
            )
            scaler_data = scaler_blob.download_blob().readall()
            
            # Load models
            import io
            test_model = joblib.load(io.BytesIO(model_data))
            test_scaler = joblib.load(io.BytesIO(scaler_data))
            
            # Test prediction
            test_input = {
                'DAYS_SINCE_EPOCH': (datetime.now() - datetime(1970, 1, 1)).days,
                'JOURNAL_COUNT': 15,
                'JOURNAL_TOTAL_AMOUNT': 50000,
                'JOURNAL_AVG_AMOUNT': 3333.33,
                'JOURNAL_STD_AMOUNT': 500,
                'MONTH': 6,
                'QUARTER': 2,
                'DAY_OF_WEEK': 1,
                'IS_WEEKEND': 0,
                'BALANCE_LAG_1': 100000,
                'BALANCE_LAG_7': 98000
            }
            
            # Prepare feature vector
            feature_vector = []
            for col in self.feature_columns:
                feature_vector.append(test_input.get(col, 0))
            
            # Scale and predict
            feature_vector = np.array(feature_vector).reshape(1, -1)
            feature_vector_scaled = test_scaler.transform(feature_vector)
            prediction = test_model.predict(feature_vector_scaled)[0]
            
            logger.info(f"‚úÖ Model test successful!")
            logger.info(f"üéØ Test prediction: ${prediction:,.2f}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Model test failed: {e}")
            return False

    def run_complete_pipeline(self):
        """Run the complete training and upload pipeline"""
        logger.info("üöÄ Starting complete training pipeline...")
        
        try:
            # Step 1: Create sample data
            balance_records, journal_entries = self.create_sample_data()
            
            # Step 2: Upload sample data to Azure
            if self.blob_service_client:
                self.upload_sample_data(balance_records, journal_entries)
            
            # Step 3: Process data
            processed_df = self.process_data_for_training(balance_records, journal_entries)
            
            # Step 4: Train model
            results = self.train_model(processed_df)
            
            # Step 5: Upload models to Azure
            if self.blob_service_client:
                upload_success = self.upload_models_to_azure()
                if upload_success:
                    self.test_uploaded_model()
            
            logger.info("üéâ Training pipeline completed successfully!")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Pipeline failed: {e}")
            return False

def main():
    """Main execution function"""
    print("ü§ñ Market Analysis Model Training Pipeline")
    print("=" * 50)
    
    # Check environment
    if not os.getenv('AZURE_STORAGE_CONNECTION_STRING'):
        print("‚ö†Ô∏è AZURE_STORAGE_CONNECTION_STRING not found in environment")
        print("Models will be trained locally but not uploaded to Azure")
    
    # Run training
    trainer = ModelTrainerUploader()
    success = trainer.run_complete_pipeline()
    
    if success:
        print("\nüéâ SUCCESS! Your model is ready!")
        print("Next steps:")
        print("1. Deploy your API if not already done")
        print("2. Test your API: python test_api.py")
    else:
        print("\n‚ùå Training failed. Check logs above.")

if __name__ == "__main__":
    main()

#============================================================================
# quick_start.py - Simplified version for your setup
#============================================================================
import os
import requests
import json
import time
from datetime import datetime
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class QuickStartTester:
    """Quick testing and demo of the API"""
    
    def __init__(self, base_url=None, api_key=None):
        # Try to detect from environment or files
        self.base_url = base_url or self._detect_api_url()
        self.api_key = api_key or self._detect_api_key()
        
        if not self.base_url or not self.api_key:
            print("‚ùå Could not detect API URL or key. Please provide them manually.")
            print("Usage: python quick_start.py")
            print("Or set environment variables: API_BASE_URL and API_KEY")
            exit(1)
        
        self.session = requests.Session()
        self.session.headers.update({
            'X-API-Key': self.api_key,
            'Content-Type': 'application/json'
        })
        
        print(f"üéØ Testing API at: {self.base_url}")
    
    def _detect_api_url(self):
        """Detect API URL from various sources"""
        # Try environment
        url = os.getenv('API_BASE_URL')
        if url:
            return url.rstrip('/')
        
        # Try deployment info file
        try:
            with open('deployment_info.txt', 'r') as f:
                for line in f:
                    if 'URL:' in line and 'https://' in line:
                        return 'https://' + line.split('https://')[-1].strip()
        except:
            pass
        
        # Default for local
        return "http://localhost:8000"
    
    def _detect_api_key(self):
        """Detect API key from various sources"""
        # Try environment
        key = os.getenv('API_KEY')
        if key:
            return key
        
        # Try .env file
        try:
            with open('.env', 'r') as f:
                for line in f:
                    if line.startswith('API_KEY='):
                        return line.split('=', 1)[1].strip()
        except:
            pass
        
        return None
    
    def test_health(self):
        """Test health endpoint"""
        print("\nüè• Testing Health Check...")
        try:
            response = self.session.get(f"{self.base_url}/")
            response.raise_for_status()
            data = response.json()
            
            print(f"‚úÖ API is healthy")
            print(f"   Status: {data.get('status')}")
            print(f"   Model Loaded: {data.get('model_loaded')}")
            print(f"   OpenAI Configured: {data.get('openai_configured')}")
            return True
        except Exception as e:
            print(f"‚ùå Health check failed: {e}")
            return False
    
    def test_prediction(self):
        """Test basic prediction"""
        print("\nüîÆ Testing Natural Language Prediction...")
        
        test_cases = [
            "I have 15 journal entries worth $50,000",
            "20 transactions totaling $75,000 on Tuesday",
            "5 entries, $25,000 total, Friday"
        ]
        
        for i, test_input in enumerate(test_cases, 1):
            try:
                response = self.session.post(
                    f"{self.base_url}/api/predict-nl",
                    json={"input": test_input}
                )
                response.raise_for_status()
                data = response.json()
                
                print(f"‚úÖ Test {i}: {test_input[:30]}...")
                print(f"   Predicted: ${data.get('predicted_balance', 0):,.2f}")
            except Exception as e:
                print(f"‚ùå Test {i} failed: {e}")
    
    def test_chat(self):
        """Test chat functionality"""
        print("\nüí¨ Testing AI Chat...")
        
        try:
            response = self.session.post(
                f"{self.base_url}/api/chat",
                json={"question": "What should I monitor for cash flow management?"}
            )
            response.raise_for_status()
            data = response.json()
            
            print("‚úÖ Chat working")
            print(f"   Response: {data.get('answer', '')[:100]}...")
        except Exception as e:
            print(f"‚ùå Chat test failed: {e}")
    
    def run_all_tests(self):
        """Run all tests"""
        print("üöÄ Market Analysis API - Quick Test Suite")
        print("=" * 50)
        
        tests = [
            self.test_health,
            self.test_prediction,
            self.test_chat
        ]
        
        passed = 0
        for test in tests:
            try:
                if test():
                    passed += 1
            except:
                pass
        
        print(f"\nüìä Results: {passed}/{len(tests)} tests passed")
        
        if passed == len(tests):
            print("üéâ All tests passed! Your API is working perfectly.")
        else:
            print("‚ö†Ô∏è Some tests failed. Check your configuration.")

def main():
    """Main function"""
    import sys
    
    # Parse command line arguments
    base_url = None
    api_key = None
    
    if len(sys.argv) > 1:
        base_url = sys.argv[1]
    if len(sys.argv) > 2:
        api_key = sys.argv[2]
    
    # Run tests
    tester = QuickStartTester(base_url, api_key)
    tester.run_all_tests()

if __name__ == "__main__":
    main()

#============================================================================
# setup_local.py - Local development setup script
#============================================================================
import os
import subprocess
import sys
from pathlib import Path

def setup_local_environment():
    """Setup local development environment"""
    print("üîß Setting up local development environment...")
    
    # Create virtual environment
    if not Path("venv").exists():
        print("üì¶ Creating virtual environment...")
        subprocess.run([sys.executable, "-m", "venv", "venv"])
    
    # Activate virtual environment and install dependencies
    if os.name == 'nt':  # Windows
        pip_path = "venv/Scripts/pip"
        python_path = "venv/Scripts/python"
    else:  # Unix/Linux/macOS
        pip_path = "venv/bin/pip"
        python_path = "venv/bin/python"
    
    print("üìö Installing dependencies...")
    subprocess.run([pip_path, "install", "-r", "requirements.txt"])
    
    # Create .env file if it doesn't exist
    if not Path(".env").exists():
        print("üìù Creating .env file...")
        with open(".env", "w") as f:
            f.write("""# Local development configuration
# Copy your Azure values here after deployment

AZURE_STORAGE_CONNECTION_STRING=your-connection-string
OPENAI_API_KEY=your-openai-key
OPENAI_API_BASE=https://your-resource.openai.azure.com/
OPENAI_API_VERSION=2024-02-15-preview
OPENAI_DEPLOYMENT_NAME=gpt-4
CONTAINER_NAME=journaldata
API_KEY=local-dev-key
PORT=8000
DEBUG=True
""")
        print("‚úÖ .env file created. Please update with your Azure values.")
    
    # Create sample data for local testing
    print("üìä Creating sample data...")
    subprocess.run([python_path, "train_and_upload_model.py"])
    
    print("‚úÖ Local environment setup complete!")
    print("\nNext steps:")
    print("1. Update .env file with your Azure configuration")
    print("2. Run the API: python app.py")
    print("3. Test the API: python quick_start.py")

if __name__ == "__main__":
    setup_local_environment()

#============================================================================
# check_requirements.py - Verify all requirements are met
#============================================================================
import subprocess
import sys
import importlib
import os

def check_python_version():
    """Check Python version"""
    version = sys.version_info
    if version.major == 3 and version.minor >= 9:
        print(f"‚úÖ Python {version.major}.{version.minor}.{version.micro}")
        return True
    else:
        print(f"‚ùå Python {version.major}.{version.minor}.{version.micro} (requires 3.9+)")
        return False

def check_packages():
    """Check required packages"""
    required_packages = [
        'flask', 'pandas', 'numpy', 'scikit-learn', 
        'joblib', 'requests', 'python-dotenv'
    ]
    
    missing = []
    for package in required_packages:
        try:
            importlib.import_module(package.replace('-', '_'))
            print(f"‚úÖ {package}")
        except ImportError:
            print(f"‚ùå {package}")
            missing.append(package)
    
    return len(missing) == 0, missing

def check_environment():
    """Check environment configuration"""
    required_env = [
        'AZURE_STORAGE_CONNECTION_STRING',
        'OPENAI_API_KEY', 
        'API_KEY'
    ]
    
    missing = []
    for env_var in required_env:
        if os.getenv(env_var):
            print(f"‚úÖ {env_var}")
        else:
            print(f"‚ùå {env_var}")
            missing.append(env_var)
    
    return len(missing) == 0, missing

def check_azure_cli():
    """Check Azure CLI"""
    try:
        result = subprocess.run(['az', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ Azure CLI")
            return True
        else:
            print("‚ùå Azure CLI")
            return False
    except FileNotFoundError:
        print("‚ùå Azure CLI not found")
        return False

def main():
    """Main check function"""
    print("üîç Checking Requirements for Market Analysis API")
    print("=" * 50)
    
    checks = [
        ("Python Version", check_python_version),
        ("Azure CLI", check_azure_cli),
        ("Python Packages", lambda: check_packages()[0]),
        ("Environment Variables", lambda: check_environment()[0])
    ]
    
    all_passed = True
    
    for check_name, check_func in checks:
        print(f"\n{check_name}:")
        passed = check_func()
        if not passed:
            all_passed = False
    
    print("\n" + "=" * 50)
    if all_passed:
        print("üéâ All requirements met! You're ready to deploy.")
    else:
        print("‚ö†Ô∏è Some requirements are missing. Please install them before deploying.")
        print("\nQuick fixes:")
        print("‚Ä¢ Install packages: pip install -r requirements.txt")
        print("‚Ä¢ Install Azure CLI: https://docs.microsoft.com/en-us/cli/azure/install-azure-cli")
        print("‚Ä¢ Set environment variables in .env file")

if __name__ == "__main__":
    main()
